# Generative Supervised Fine-tuning of GPT-2

Today we'll be working through a notebook focused on fine-tuning GPT-2 to get better at SQL generation!

### Prompt Engineering 101: ICL and CoT

This [notebook](https://colab.research.google.com/drive/1ComI9fkxUFL_DBgCkIj4kXFEeidqjmtK?usp=sharing) will walk through the basics of how In-Context Learning (ICL) and Chain of Thought Prompting (CoT) can be leveraged to increase the output of your LLMs.

## Build üèóÔ∏è

We will fine-tune GPT-2 on generating more cohere SQL from a natural language query and a provided context. 

You can either follow the: 

- [Assignment Notebook](https://colab.research.google.com/drive/1ZRKhOwvFS3aHahrt97C6wF5YQ3nIdUNB?usp=sharing)
- [BEASTMODE Notebook](https://colab.research.google.com/drive/1yG1GYu3M5t3QTsY-Sv7Rgs4m2RPMdzf0?usp=sharing)

## Ship üö¢

Ship a completed notebook to us! There are a few questions found throughout the notebook that will need to be answered.

Provide a URL to your loom video walkthrough

## Share üöÄ

Share about your experience in a LinkedIn post, or in the Discord!

Submit your homework direclty [here](https://docs.google.com/forms/d/e/1FAIpQLSfhEa9Qh3ezmNKar2oaKSp9-qmadupS5SHqMii3cjuln7cwtQ/viewform?usp=sf_link)