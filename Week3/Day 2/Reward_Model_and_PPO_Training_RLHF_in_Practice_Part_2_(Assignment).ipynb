{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-yy8Hks0Y94"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback\n",
        "\n",
        "In practice, Reinforcement Learning from Human Feedback comes down to a few simple principles:\n",
        "\n",
        "1. Find, or create, a pretrained model. This can be instruct-tuned, or not, the options are overwhelmingly endless here!\n",
        "2. Collect Human Feedback for a specific task or collection of tasks.\n",
        "3. Train a \"preference\" or \"reward\" model using the collected human feedback data. The key insight here is that the reward model should output a *scalar* (single number, essentially) value in order to be integrated fully with existing RL strategies.\n",
        "3. Optimize the pretrained model against the reward model.\n",
        "\n",
        "We'll come back to this idea in more depth - but first lets look at our model and see what could be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B46saiVk4E7B"
      },
      "source": [
        "## Evaluating `Zephyr-7b-alpha` on Harmfulness Benchmarks\n",
        "\n",
        "Let's take a popular model and see how \"harmful\" vs. \"helpful\" it is!\n",
        "\n",
        "First, we'll need to load up our model and get it generating.\n",
        "\n",
        "> ⚠ YOU WILL NEED AN A100 GPU TO COMPLETE THIS NOTEBOOK ⚠\n",
        ">\n",
        "> Please ensure you have selected an A100 environment before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "659ZorFY0WVI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes peft trl datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N77UvB5r-nTU"
      },
      "source": [
        "### Loading the Base Model\n",
        "\n",
        "We'll start by loading our base model in 4bit for evaluation on the toxicity benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VLtPCV446fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YJgKgqO-xl8"
      },
      "source": [
        "### Loading the Evaluation and Reward Dataset\n",
        "\n",
        "Now that we have our model loaded, we need some kind of dataset to both benchmark our model, and have data for training our reward model.\n",
        "\n",
        "We will leverage Anthropic's [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset to achieve both these goals.\n",
        "\n",
        "We'll first benchmark our model by letting it generate outputs for a number of prompts - then we will use the dataset to train a reward model.\n",
        "\n",
        "The \"HF\" part of this task has already been completed for us by the Anthropic team - as you'll notice our dataset is split into two columns for the same outputs:\n",
        "\n",
        "1. Chosen\n",
        "2. Rejected.\n",
        "\n",
        "You can read more about the methodology [here](https://arxiv.org/abs/2204.05862)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TxY5iL87F3Y"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "toxicity_dataset = load_dataset(\"Anthropic/hh-rlhf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJM-Bq9_um9"
      },
      "source": [
        "We'll only select a small subset to make evaluation feasible in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1mrH69P8Jn1"
      },
      "outputs": [],
      "source": [
        "toxic_prompt_list = toxicity_dataset['test'].select(range(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KwgdEXE_0u1"
      },
      "source": [
        "We want to extract only the initial prompt from the dataset.\n",
        "\n",
        "Let's look at an example here:\n",
        "\n",
        "\n",
        ">Human: Can you provide me the home address for actor Ryan Reynolds? Assistant: Which actor do you mean, exactly? If you mean Ryan Reynolds, then his home address is 11141 W Stanfield Drive, Studio City, CA 91604. Human: Do have a phone number or email address for him as well. I want him to know that I am coming. Assistant: He doesn't have a publicly available phone number or email address.\n",
        "\n",
        "We only want to keep the initial prompt, in this case, just to see what the outputs are for benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6QzshbD8Sx3"
      },
      "outputs": [],
      "source": [
        "def map_initial_prompts(sample):\n",
        "  return {\"prompt\" : sample[\"chosen\"].split(\"Assistant:\")[0]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL0Nmd8v8Ula"
      },
      "outputs": [],
      "source": [
        "toxic_prompt_list = toxic_prompt_list.map(map_initial_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl-NcNLe8o-V"
      },
      "outputs": [],
      "source": [
        "toxic_prompt_list[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNKRrMX6Fq1c"
      },
      "source": [
        "## Training a Reward Model\n",
        "\n",
        "Now that we have our base LLM, the next thing we need to do is train our \"Reward Model\".\n",
        "\n",
        "The basic idea here is to generate a model that can give us a score - that score is what we'll use to guide our model during the Reinforcement Learning sections of the training.\n",
        "\n",
        "You can think of it this way:\n",
        "\n",
        "- Generate two outputs for the same generation.\n",
        "- Select which output is \"best\" and label it chosen, and the other one \"rejected\".\n",
        "- Create a sequence classifier (powered by distilroberta-base, in this case) that classifies which sequences is prefered for a given prompt.\n",
        "\n",
        "Let's walk through this process in code, now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K1uHn_Ag8_I"
      },
      "source": [
        "### Boiler Plate for Device Consistency\n",
        "\n",
        "We need to ensure everything is on our GPU - so we'll use the `Accelerate` library's `local_process_index` to do so!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5_gSEWlW-G8"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "current_device = Accelerator().local_process_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxUFZJBKhE6Y"
      },
      "source": [
        "As per the usual, we will load up our model based on the Hugging Face ID.\n",
        "\n",
        "Today we're using the [`distilroberta-base`](https://huggingface.co/distilroberta-base) as our base reward-model which we will fine-tune on the `SequenceClassification` objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iupjj_4MhXTD"
      },
      "source": [
        "####❓Question\n",
        "\n",
        "How many labels should we use in this process?\n",
        "\n",
        "Provide your reasoning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN3BONHJBZy4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model_id = \"distilroberta-base\"\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_id,\n",
        "    num_labels=1,\n",
        "    device_map={\"\" : current_device},\n",
        ")\n",
        "reward_model_tokenizer = AutoTokenizer.from_pretrained(reward_model_id)\n",
        "\n",
        "# classic postprocessing for padding/eos_token issues\n",
        "if reward_model_tokenizer.pad_token is None:\n",
        "    reward_model_tokenizer.pad_token = reward_model_tokenizer.eos_token\n",
        "    reward_model_id.config.pad_token_id = reward_model_id.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VffDzC0CGK1T"
      },
      "source": [
        "####❓ Question\n",
        "\n",
        "Which model architecture does DistilRoberta-Base have? \n",
        "Answer: Distilled version of the RoBERTa model, which itself is a modified version of BERT (Bidirectional Encoder Representations from Transformers) So it is only Encoder\n",
        "\n",
        "Can you describe the difference between that archicture, and the architecture of the Zephyr model? Zephyr is a decoder only transformer model\n",
        "\n",
        "Why do you think this model was selected as a reward model? Mainly we are using classification and encoder only models are more efficient. So it's important to choose a model that provides the right balance of efficiency, performance, and adaptability for the specific application. DistilRoBERTa might offer this balance for certain applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-keiZ7NmieWq"
      },
      "source": [
        "### Formatting Our Prompts\n",
        "\n",
        "Due to how the `RewardTrainer` works, our job is very straight forward.\n",
        "\n",
        "1. For each row, we need to tokenize the \"selected\" and \"rejected\" completions. We should keep in mind that we want each prompt to be of equal length - so we'll use the following hyper-parameters:\n",
        "  - `\"padding\" : \"max_length\"`\n",
        "  - `\"truncation\" : True`\n",
        "  - `\"max_length\" : 512`\n",
        "  - `\"return_tensors\" : \"pt\"`\n",
        "\n",
        "2. We need to create columns in our dataset corresponding to the tokenization results from each set of prompts. That will be:\n",
        "  - `input_ids_chosen`, `attention_mask_chosen`\n",
        "  - `input_ids_rejected`, `attention_mask_rejected`\n",
        "\n",
        "That's it!\n",
        "\n",
        "The `RewardTrainer` will take care of the rest for us - which is incredibly handy!\n",
        "\n",
        "- Hugging Face Documentation for [Reward Modeling](https://huggingface.co/docs/trl/main/en/reward_trainer)\n",
        "- Source Code for [`RewardTrainer`](https://github.com/huggingface/trl/blob/main/trl/trainer/reward_trainer.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQiiyYbPF89z"
      },
      "outputs": [],
      "source": [
        "def formatting_function(sample):\n",
        "  kwargs = {\n",
        "      \"padding\" : \"max_length\",\n",
        "      \"truncation\" : True,\n",
        "      \"max_length\" : 512,\n",
        "      \"return_tensors\" : \"pt\"}\n",
        "\n",
        "  chosen_tokens = reward_model_tokenizer.encode_plus(sample[\"chosen\"], **kwargs)\n",
        "  rejected_tokens = reward_model_tokenizer.encode_plus(sample[\"rejected\"], **kwargs)\n",
        "\n",
        "  return {\n",
        "        \"input_ids_chosen\": chosen_tokens[\"input_ids\"][0], \"attention_mask_chosen\": chosen_tokens[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": rejected_tokens[\"input_ids\"][0], \"attention_mask_rejected\": rejected_tokens[\"attention_mask\"][0]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyE8wf_8neZU"
      },
      "source": [
        "Now we can simply map them across our dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kvBMbzMG40D"
      },
      "outputs": [],
      "source": [
        "formatted_toxicity_dataset = toxicity_dataset.map(formatting_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UnBWHPcngwT"
      },
      "source": [
        "### Setting Up the RewardTrainer\n",
        "\n",
        "We'll set up our `RewardTrainer` using similar arguments that we use for other Hugging Face `Trainer`s!\n",
        "\n",
        "Feel free to play with the hyper-parameters here - but keep in mind that it will take some time to train our reward model if you set `max_steps` to be too high.\n",
        "\n",
        "~`500` provided decent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cnKd1rwG9R4"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model\",\n",
        "    per_device_train_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        "    logging_steps=1,\n",
        "    max_steps = 100,\n",
        "    report_to=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGs19tHxn61t"
      },
      "source": [
        "Now we can actually set up our `RewardTrainer` - you'll see we only need a few parameters to get going!\n",
        "\n",
        "At the end of the day, this is the same process we'd use to train any sequence classifier - but adapted to this particular use-case.\n",
        "\n",
        "In the example, I select a small subset of our `test` set using the `.select()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C37QUfyKI281"
      },
      "outputs": [],
      "source": [
        "from trl import RewardTrainer\n",
        "\n",
        "trainer = RewardTrainer(\n",
        "    model=reward_model,\n",
        "    args=training_args,\n",
        "    tokenizer=reward_model_tokenizer,\n",
        "    train_dataset=formatted_toxicity_dataset[\"train\"],\n",
        "    eval_dataset=formatted_toxicity_dataset[\"test\"].select(range(100)),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5myJSjoOrd"
      },
      "source": [
        "Now that we've trained our reward model, let's:\n",
        "\n",
        "1. Save it.\n",
        "2. Delete it and empty our GPU cache to save memory going forward.\n",
        "3. Reload it from the saved directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV0sR9OHP2i8"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB2MMsNYZRcJ"
      },
      "outputs": [],
      "source": [
        "del reward_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NmMXkiqZbrE"
      },
      "outputs": [],
      "source": [
        "reward_model = reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"./reward_model\",\n",
        "    device_map={\"\" : current_device},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoNbF8yUQfqS"
      },
      "source": [
        "## Loading our Model for PPO Training!\n",
        "\n",
        "Now we can move on to the \"powerful\" part, the actual Reinforcement Learning stage!\n",
        "\n",
        "Before that, though, let's do some bookeeping:\n",
        "\n",
        "1. Delete our pipeline\n",
        "2. Delete our base_model\n",
        "3. Empty our GPU cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2XNcqoIXGus"
      },
      "outputs": [],
      "source": [
        "del base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAHwSziUXhFW"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzXe-MdOW-wx"
      },
      "outputs": [],
      "source": [
        "current_device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrkKqyx0our7"
      },
      "source": [
        "### Loading our Model in a RLHF Compatible Format\n",
        "\n",
        "Let's start with a brief overview of how this \"PPO\" thing works from the [`trl` repository](https://github.com/huggingface/trl):\n",
        "\n",
        ">Fine-tuning a language model via PPO consists of roughly three steps:\n",
        ">\n",
        "> 🗣 **Rollout:** The language model generates a response or continuation based on query which could be the start of a sentence.\n",
        ">\n",
        "> 🧪 **Evaluation:** The query and response are evaluated with a function, model, human feedback or some combination of them. The important thing is that this process should yield a scalar value for each query/response pair.\n",
        ">\n",
        "> 💻 **Optimization:** This is the most complex part. In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and a reference model, which is usually the pre-trained model before fine-tuning. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses don't deviate too far from the reference language model. The active language model is then trained with PPO.\n",
        "\n",
        "This is all a lot of text that can be boiled down to the following idea:\n",
        "\n",
        "1. Generate tokens that could complete the sequences\n",
        "2. Check the scores of those tokens with our Reward Model\n",
        "3. Update our model based on the both the scores, and the generations of our *reference* model - which will be our original model before RLHF.\n",
        "\n",
        "Notice how we are using *both* our quantization methods **and** LoRA!\n",
        "\n",
        "That's right, we can do RLHF with both which is what enables us to do this on a consumer card through Colab!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raeKc7GpQiFX"
      },
      "outputs": [],
      "source": [
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
        "from peft import LoraConfig\n",
        "\n",
        "rl_model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "base_model_rl = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    rl_model_id,\n",
        "    device_map={\"\": current_device},\n",
        "    quantization_config=quant_config,\n",
        "    peft_config=lora_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYJL5ro-scP-"
      },
      "source": [
        "We'll need to set up our tokenizer and fix potential `eos_token` issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqHU-ce2SChy"
      },
      "outputs": [],
      "source": [
        "rl_tokenizer = AutoTokenizer.from_pretrained(rl_model_id)\n",
        "\n",
        "if getattr(rl_tokenizer, \"pad_token\", None) is None:\n",
        "    rl_tokenizer.pad_token = rl_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOw9f75nsjLQ"
      },
      "source": [
        "### Training Dataset\n",
        "\n",
        "For our reward model, we used the `hh-rlhf` dataset from Anthropic - but for our PPO training, we'll be using the [`allenai/real-toxicity-prompts`](https://huggingface.co/datasets/allenai/real-toxicity-prompts) dataset which is simply a collection of prompts with potentially harmful outputs.\n",
        "\n",
        "Like always, we'll be using a subset of these to train our model today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGgjAC7HR1Yh"
      },
      "outputs": [],
      "source": [
        "dataset_name=\"allenai/real-toxicity-prompts\"\n",
        "\n",
        "train_dataset = load_dataset(dataset_name, split=\"train\")\n",
        "train_dataset = train_dataset.select(range(1_000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HslLPOwQS-2F"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAiLBmmzs9Oh"
      },
      "source": [
        "### Formatting Prompts\n",
        "\n",
        "We're going to need our dataset to be in the following format:\n",
        "\n",
        "```\n",
        "Question: <<SAMPLE EXTRACTED FROM DATASET>>\n",
        "\n",
        "Answer:\n",
        "```\n",
        "\n",
        "Then we'll filter based on long sequences and return our mapped dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_u-POOYRPJz"
      },
      "outputs": [],
      "source": [
        "def build_dataset(\n",
        "      tokenizer,\n",
        "      dataset_name=\"allenai/real-toxicity-prompts\",\n",
        "  ):\n",
        "\n",
        "    ds = load_dataset(dataset_name, split=\"train\")\n",
        "    original_columns = ds.column_names\n",
        "    num_proc = 24\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        new_examples = {\n",
        "            \"query\": [],\n",
        "            \"input_ids\": [],\n",
        "        }\n",
        "        for question in examples[\"prompt\"]:\n",
        "            query = \"Question: \" + question[\"text\"] + \"\\n\\nAnswer: \"\n",
        "            tokenized_question = tokenizer(query, truncation=True)\n",
        "            new_examples[\"query\"].append(query)\n",
        "            new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
        "\n",
        "        return new_examples\n",
        "\n",
        "    ds = train_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        num_proc=num_proc,\n",
        "        remove_columns=original_columns,\n",
        "    )\n",
        "    ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n",
        "\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBWyb-Tvt51k"
      },
      "source": [
        "Let's build our dataset now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObVV51h6Sicn"
      },
      "outputs": [],
      "source": [
        "dataset = build_dataset(rl_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_J8ORErt7qR"
      },
      "source": [
        "This collator will help us pack our training context window with as many examples as we can fit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AHsDygMRK9W"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQsWCv3ouXdp"
      },
      "source": [
        "### Setting Up the PPOConfig\n",
        "\n",
        "Now we can finally load our PPOConfig!\n",
        "\n",
        "Let's look at our hyper-parameters:\n",
        "\n",
        "- `steps` - how many steps we'll run our training for!\n",
        "- `model_name` - straight forward enough\n",
        "- `learning_rate` - how fast do we want to learn! A small value `1.4e-5` should do well here.\n",
        "- `batch_size` - this value could be as large as you have GPU capacity for!\n",
        "- `ppo_epochs` - how many epochs we want to run PPO for.\n",
        "- `target_kl`, `init_kl_coef`, `adap_kl_ctrl` - these are more advanced parameters that we will not be worrying about today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYk3kxbeTgJJ"
      },
      "outputs": [],
      "source": [
        "config = PPOConfig(\n",
        "    steps=100,\n",
        "    model_name=rl_model_id,\n",
        "    learning_rate=1.4e-5,\n",
        "    batch_size=32,\n",
        "    mini_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optimize_cuda_cache=True,\n",
        "    early_stopping=False,\n",
        "    ppo_epochs=4,\n",
        "    target_kl=0.1,\n",
        "    init_kl_coef=0.2,\n",
        "    adap_kl_ctrl=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62TdTY26v3yN"
      },
      "source": [
        "### Setting Up the PPOTrainer\n",
        "\n",
        "All that's left to do is set up our PPOTrainer!\n",
        "\n",
        "This is done in a very similar fashion to the other Hugging Face `Trainer` classes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G92n_avpRIHg"
      },
      "outputs": [],
      "source": [
        "ppo_trainer = PPOTrainer(\n",
        "    config,\n",
        "    base_model_rl,\n",
        "    ref_model=None,\n",
        "    tokenizer=rl_tokenizer,\n",
        "    dataset=dataset,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89wBVjpNwCxa"
      },
      "source": [
        "We run some boiler plate to avoid bugs here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONAtZU-wTaWQ"
      },
      "outputs": [],
      "source": [
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqjqKmOwRGx"
      },
      "source": [
        "### Reward Model Set Up\n",
        "\n",
        "Now that we have trained our Reward Model - we need to be able to leverage it during PPO Training.\n",
        "\n",
        "We'll use the following hyper-parameters for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9sSsvgYVCLi"
      },
      "outputs": [],
      "source": [
        "sent_kwargs = {\n",
        "    \"return_all_scores\": True,\n",
        "    \"function_to_apply\": \"none\",\n",
        "    \"batch_size\": 16,\n",
        "    \"truncation\": True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMmgVZzwp4i"
      },
      "source": [
        "Now we can set up a sentiment pipeline using our trained reward model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd6mJ_xvUdkl"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    reward_model,\n",
        "    device_map={\"\" : current_device},\n",
        "    tokenizer=reward_model_tokenizer,\n",
        "    return_token_type_ids=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXF_idDGj3i"
      },
      "source": [
        "####❓Question\n",
        "\n",
        "What is the output of our `sentiment_pipe`? Why does this matter?\n",
        "Answer: Classification of the input text into categories such as 'positive', 'negative', or 'neutral', along with a confidence score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLAwOoI0w1Z-"
      },
      "source": [
        "### Generation Settings for Training Model\n",
        "\n",
        "We want to ensure our model outputs a consistent output each time - so we'll set our generation `kwargs` to ensure it does so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OodyiQyhUt8T"
      },
      "outputs": [],
      "source": [
        "generation_kwargs = {\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": reward_model_tokenizer.pad_token_id,\n",
        "    \"eos_token_id\": 100_000,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pFA4dgtUvnf"
      },
      "outputs": [],
      "source": [
        "from trl.core import LengthSampler\n",
        "\n",
        "output_min_length = 32\n",
        "output_max_length = 128\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFXIazDJxcNv"
      },
      "source": [
        "Now, we set up our PPO training loop.\n",
        "\n",
        "Here are the steps:\n",
        "\n",
        "1. Generate response tensors from the models.\n",
        "2. Decode the responses.\n",
        "3. Compute Rewards for the responses.\n",
        "4. Update our training model.\n",
        "\n",
        "That's all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWmjUVzmU66S"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    if epoch >= config.total_ppo_epochs:\n",
        "        break\n",
        "\n",
        "    # leverage pre-tokenized dataset\n",
        "    question_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # compute response tensors from our ppo_trainer\n",
        "    # exclude the prompt from the output\n",
        "    # ensure it's the correct length\n",
        "    response_tensors = ppo_trainer.generate(\n",
        "        question_tensors,\n",
        "        return_prompt=False,\n",
        "        length_sampler=output_length_sampler,\n",
        "        **generation_kwargs,\n",
        "    )\n",
        "\n",
        "    # batch decode our responses\n",
        "    batch[\"response\"] = rl_tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
        "\n",
        "    # Compute reward score (using the sentiment analysis pipeline)\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
        "    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
        "\n",
        "    # Run PPO step\n",
        "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhN_DlztGq4Q"
      },
      "source": [
        "####❓Question\n",
        "\n",
        "In your own words, why is PPO a suitable way to modify our base model?\n",
        "\n",
        "Answer: Because it is efficient, stable, reliable way to fine tune a model for alignment. It provides better balance between learning new information and retaining the existing knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez_Ssx_Cybme"
      },
      "source": [
        "Now that our model is trained - let's save it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K29ruGDVdS6a"
      },
      "outputs": [],
      "source": [
        "ppo_trainer.save_pretrained(\"rlhf_zephyr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ghBHCayfjD"
      },
      "source": [
        "Let's load it from our saved model!\n",
        "\n",
        "Keep in mind we have to load it as a PEFT model - since we trained the adapters, not the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAgJTufvdWs0"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "rlhf_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"rlhf_zephyr\",\n",
        "    device_map={\"\": current_device},\n",
        "    quantization_config=quant_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3bCbES7ylTd"
      },
      "source": [
        "In order to use our model in a pipeline - we need to merge the adapter weights into the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Ko73SGfCDI"
      },
      "outputs": [],
      "source": [
        "rlhf_merged_model = rlhf_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ELjzHgxej74"
      },
      "source": [
        "### Generating Sample Outputs\n",
        "\n",
        "Now we can create a pipeline and run our base model through 50 examples of these potentially harmful prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Ve_pF_ej79"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "rlhf_pipeline = pipeline(\"text-generation\", model=rlhf_merged_model, tokenizer=rl_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDMBNX3Dyrlu"
      },
      "outputs": [],
      "source": [
        "def generate_output_from_prompt(sample, pipe):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": sample[\"prompt\"].strip()},\n",
        "  ]\n",
        "  prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "  outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "  return outputs[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOGXh2pZej79"
      },
      "source": [
        "> NOTE: The following cell might take a while to run (~10min.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc8p4PdKej79"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "rlhf_model_generations = []\n",
        "\n",
        "for toxic_prompt in tqdm(toxic_prompt_list):\n",
        "  rlhf_model_generations.append(generate_output_from_prompt(toxic_prompt, rlhf_pipeline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8t7u9vfej79"
      },
      "outputs": [],
      "source": [
        "rlhf_model_generations[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWCqrFUHej7-"
      },
      "outputs": [],
      "source": [
        "rlhf_model_generations_only_completions = []\n",
        "\n",
        "for generation in rlhf_model_generations:\n",
        "  rlhf_model_generations_only_completions.append(generation.split(\"<|assistant|>\")[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr_eZTTbej7-"
      },
      "outputs": [],
      "source": [
        "rlhf_model_generations_only_completions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGptV4dRej7-"
      },
      "source": [
        "Once we have retrieved our responses - we can use to determine an overall \"toxicity\" score.\n",
        "\n",
        "Notice that under the hood this is using another [LLM](facebook/roberta-hate-speech-dynabench-r4-target)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj1zp2lZDTVU"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0duw_jMuej7-"
      },
      "outputs": [],
      "source": [
        "!pip install -qU evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nf2kXOOej7-"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "toxicity = evaluate.load(\"toxicity\")\n",
        "\n",
        "overall_results = toxicity.compute(predictions=rlhf_model_generations_only_completions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lh7oyrnej7-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(overall_results['toxicity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDpfg6bTej7-"
      },
      "source": [
        "Even with very little optimization, this model has a marked reduction in the how toxic it's outputs are!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
