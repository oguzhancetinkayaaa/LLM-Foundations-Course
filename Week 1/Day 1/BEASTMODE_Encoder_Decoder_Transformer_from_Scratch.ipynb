{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZwKP3f2HGz9"
      },
      "source": [
        "# Encoder-Decoder Transformer Model from Scratch in PyTorch\n",
        "\n",
        "In today's notebook, we'll be focusing on two major components of transformers:\n",
        "\n",
        "1. The Building Blocks\n",
        "2. Training the Model\n",
        "\n",
        "As a brunt of the in-class time will be spent on the building blocks, we'll leave the training logic as an assignment to complete. We'll be focusing more specifically on training once we start using decoder-only architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjbGDn9SF49m"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "Throughout our time together - we'll be providing a number of assignments. Each assignment will be split into two broad categories:\n",
        "\n",
        "1. Base Assignment - a more conceptual and theory based assignment focused on locking in specific key concepts and learnings.\n",
        "2. BEASTMODE Assignment - a more programming focused assignment focused on core code-concepts used in transformers.\n",
        "\n",
        "Each assignment will have a few of the following categories of exercises:\n",
        "\n",
        "1. ‚ùìQuestions - these will be questions that you will be expected to gather the answer to!\n",
        "2. üèóÔ∏è Activities - these will be work or coding activities meant to reinforce specific concepts or theory components.\n",
        "3. BEASTMODE Specific: üöß Build Exercises - these will be coding challenges!\n",
        "\n",
        "You are expected to complete all of the activities in your selected notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z43kfJGvlQyA"
      },
      "source": [
        "# The Building Block Fundamentals of Transformer Architecture\n",
        "\n",
        "We're going to start with an example of an encoder-decoder model - the kind found in the classic paper:\n",
        "\n",
        "[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "We'll walk through each step in code - leveraging the [PyTorch]() library heavily - in order to get an idea of how these models work.\n",
        "\n",
        "While this example notebook could be extended to a sincere usecase - we'll be using a toy dataset, and we will not fully train the model until it converges (under-train), as the full training process might take many days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sd2KYudl0Hn"
      },
      "source": [
        "## The Desired Architecture\n",
        "\n",
        "![image](https://i.imgur.com/YPjbqW6.png)\n",
        "\n",
        "We'll skip over the diagram for now, and talk through each component in detail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TPUnyvsuovuP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8M3oT0l8fM"
      },
      "source": [
        "## Embedding\n",
        "\n",
        "![image](https://i.imgur.com/sFlEZ2e.png)\n",
        "\n",
        "The first step will be do convert our tokenized sequence of inputs into an embedding vector. This allows use to understand a rich amount of information about input sequences and their semantic meanings.\n",
        "\n",
        "As the embedding layer will be training along side the rest of the model - it will allow us to have an excellent vector-representation of the tokens in our dataset.\n",
        "\n",
        "Let's see how it looks in code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ddpQjPJWmXZg"
      },
      "outputs": [],
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "    \"\"\"\n",
        "    vocab_size - the size of our vocabulary\n",
        "    d_model - the dimension of our embeddings and the input dimension for our model\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model) # scale embeddings by square root of d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Q4WPNALUKR"
      },
      "source": [
        "### ‚ùìQuestion 1:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `InputEmbeddings` layer be?\n",
        "\n",
        "### Answer: [16, 350, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt4lFHdCpAx9",
        "outputId": "dd39aec6-73a1-468e-a707-21af0a8e7846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 350, 512])\n"
          ]
        }
      ],
      "source": [
        "d_model = 512  # Dimension of embeddings\n",
        "vocab_size = 10000  # Size of vocabulary\n",
        "batch_size = 16  # Batch size\n",
        "seq_length = 350  # Sequence length\n",
        "\n",
        "embed = InputEmbeddings(d_model, vocab_size)\n",
        "input_indices = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length))\n",
        "output = embed(input_indices)\n",
        "\n",
        "\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra5KCa1KnfrC"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "![image](https://i.imgur.com/IIA3NK3.png)\n",
        "\n",
        "We need to impart information about where each token is in the sequence, but we aren't using any recurrence or convolutions - the easiest way to encode positional information is to inject positional information into our input embeddings.\n",
        "\n",
        "We're going to use the process outlined in the paper to do this - which is to use a specific combination of functions to add positional information to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5K3NXh7MoM5D"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    positional_embeddings = torch.zeros(seq_len, d_model)\n",
        "    positional_sequence_vector = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "    positional_model_vector = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "    positional_embeddings[:, 0::2] = torch.sin(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings[:, 1::2] = torch.cos(positional_sequence_vector * positional_model_vector)\n",
        "    positional_embeddings = positional_embeddings.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer('positional_embeddings', positional_embeddings)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.positional_embeddings[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKGHvs0eMCoQ"
      },
      "source": [
        "### ‚ùìQuestion 2:\n",
        "\n",
        "Given:\n",
        "\n",
        "1. Batch Size = `16`\n",
        "2. Sequence Length = `350`\n",
        "\n",
        "What will the output shape of the `PositionalEncoding` layer be?\n",
        "\n",
        "### Answer: [16, 350, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEhpduPKpAx9",
        "outputId": "f8a9f3f5-8227-4ed1-b2aa-d528419f0658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 350, 512])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 16\n",
        "seq_len = 350\n",
        "d_model = 512  # Example dimension, you can change this\n",
        "dropout_rate = 0.1  # Example dropout rate, you can modify this\n",
        "\n",
        "pos_encoding = PositionalEncoding(d_model, seq_len, dropout_rate)\n",
        "sample_input = torch.rand(batch_size, seq_len, d_model)\n",
        "output = pos_encoding(sample_input)\n",
        "\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueiM7LzKpcFY"
      },
      "source": [
        "## Add & Norm\n",
        "\n",
        "Next we'll tackle the Add & Norm Block of the diagram.\n",
        "\n",
        "![image](https://i.imgur.com/otdEq4D.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lDABPLSKqOt"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "The first step is to add layer normalization. You can read more about it [here](https://paperswithcode.com/method/layer-normalization)!\n",
        "\n",
        "The basic idea is that it makes training the model a bit easier, and allows the model to generalize a bit better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1Nlv7BH7ruSf"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, features: int, epsilon:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = nn.Parameter(torch.ones(features))\n",
        "    self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim = -1, keepdim = True)\n",
        "    standard_deviation = x.std(dim = -1, keepdim = True)\n",
        "    return self.gamma * (x - mean) / (standard_deviation + self.epsilon) + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg-deMc3MNnM"
      },
      "source": [
        "### ‚ùìQuestion 3:\n",
        "\n",
        "What is the purpose of `epsilon` in the above code.\n",
        "\n",
        "> HINT: Pay special attention to the math in the `return` statement.\n",
        "\n",
        "### Answer: This parameter is used to avoid division by zero during the calculation of the normalized output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXt7KKKycrl"
      },
      "source": [
        "### Residual Connection\n",
        "\n",
        "Another technique that makes model training easier, we add a Residual connection to the outputs of the Attention Block - this helps to prevent vanishing gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XKwGFD2-yd-Z"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, features: int, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layernorm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layernorm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIOZp3xhsaXK"
      },
      "source": [
        "## Feed Forward Network\n",
        "\n",
        "![image](https://i.imgur.com/woEqBjQ.png)\n",
        "\n",
        "Moving onto the next component, we have our feed forward network.\n",
        "\n",
        "The feed forward networks servers two purposes in our model:\n",
        "\n",
        "1. It reforms the attention outputs into a format that works with the next block.\n",
        "\n",
        "2. It helps add complexity to prevent each attention block acting in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JueNG2UBszbR"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1) -> None:\n",
        "    \"\"\"\n",
        "    d_model - dimension of model\n",
        "    d_ff - dimension of feed forward network\n",
        "    dropout - regularization measure\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5EGkvdtP0O"
      },
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "![image](https://i.imgur.com/4qOT46y.png)\n",
        "\n",
        "Next up is the heart and soul of the Transformer - Multi-Head Attention.\n",
        "\n",
        "We'll break it down into the basic building blocks in code in the following section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oto52ZyvX0k"
      },
      "source": [
        "### Multi-Head Attention Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mDHkw1f_vmuv"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEm7I6iwMiom"
      },
      "source": [
        "### ‚ùìQuestion 4:\n",
        "\n",
        "What do: Q, K, V, and O stand for in the above code?\n",
        "\n",
        "In your own words: What do Q, K, and V mean conceptually?\n",
        "\n",
        "### Answer\n",
        "Q: Queries. like a word in a sentence. this is what we want to compute the attention weights.\n",
        "K: Keys. This is the data that we use to compare the query. Each word in a sentence is used as a key in the attention mechanism to calculate the contribution to query.\n",
        "V: Values. Comparison result of queries and keys\n",
        "O: Output, final result, weighted sum of values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyD7nAM6tb0K"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "![image](https://i.imgur.com/Yp48DuB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj7nMiQ1NDV_"
      },
      "source": [
        "üöß Build Exercise #1:\n",
        "\n",
        "Provide the code for calculating attention.\n",
        "\n",
        "> HINT: Be sure to consider the optional mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dKk08SvowJIc"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask, d_k, dropout: nn.Dropout = None):\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac8k7b3SxKOC"
      },
      "source": [
        "### Forward Method\n",
        "\n",
        "This is code is required to do a forward pass with our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "um2Oxv1axWYK"
      },
      "outputs": [],
      "source": [
        "def forward(self, query, key, value, mask):\n",
        "  query = self.w_q(query)\n",
        "  key = self.w_k(key)\n",
        "  value = self.w_v(value)\n",
        "\n",
        "  query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "  value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "  x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "  return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgr_D0mmyEgp"
      },
      "source": [
        "### Combining it All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "odYKdmwMyH4P"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    assert d_model % num_heads == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout: nn.Dropout = None):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    query = self.w_q(query)\n",
        "    key = self.w_k(key)\n",
        "    value = self.w_v(value)\n",
        "\n",
        "    query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.num_heads * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjLhpiyz5b"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "When we pass information through our model - the first thing we will do is Encode it by passing it through our Encoder Blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0edIfMzijj"
      },
      "source": [
        "### Encoder Block\n",
        "\n",
        "![image](https://i.imgur.com/nwNYZAT.png)\n",
        "\n",
        "The encoder takes in the source language sentence (e.g. English). Each word is converted into a vector representation using an embedding layer. Then a positional encoder adds information about the position of each word. This goes through multiple self-attention layers, where each word vector attends to all other word vectors to build contextual representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dMVnZiGDy1PG"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, input_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, input_mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AvnoPrzvwu"
      },
      "source": [
        "### Encoder Stack\n",
        "\n",
        "Following along from the original paper - we will organize these blocks into a set of 6.\n",
        "\n",
        "These 6 Encoder Blocks (each with 8 Attention Heads) will comprise our Encoding Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kOaR5SjUzxv7"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQyujsRTz5NT"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Next, we will take the encoded sequence and decode it through our Decoder Blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYgl77Kz6bx"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "![image](https://i.imgur.com/HtAAXZc.png)\n",
        "\n",
        "The decoder takes in the target language sentence (e.g. Italian). It also converts words to vectors and adds positional info. Then it goes through self-attention layers. Here, a mask is applied so each word can only see the words before it, not after.\n",
        "\n",
        "The decoder also does attention over the encoder output. This allows each French word to find relevant connections with the English words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SIwafbqzz5-n"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.cross_attention_block = cross_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, input_mask))\n",
        "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4yiTNn0BkA"
      },
      "source": [
        "### Decoder Stack\n",
        "\n",
        "We'll use the same number of Decoder Blocks as we did Encoder Blocks - leaving us with 6 Deocder Blocks in our Decoder Stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1QUXzOXk0CcT"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "  def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(features)\n",
        "\n",
        "  def forward(self, x, encoder_output, input_mask, target_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, input_mask, target_mask)\n",
        "    return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRFiAP580S4b"
      },
      "source": [
        "## Linear Projection Layer\n",
        "\n",
        "After the decoder's self-attention and encoder-decoder attention layers, we have a context vector representing each Italian word. This context vector has a high dimension (e.g. 512 or 1024).\n",
        "\n",
        "We want to take this context vector and generate a probability distribution over the French vocabulary so we can pick the next translated word.\n",
        "\n",
        "The linear projection layer helps with this. It projects the context vector into a much larger vector called the vocabulary distribution - one entry per word in the vocabulary.\n",
        "\n",
        "For example, if our Italian vocabulary has 50,000 words, the vocabulary distribution will have 50,000 dimensions. Each dimension corresponds to the probability of that Italian word being the correct translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tkBBMAZK0WLB"
      },
      "outputs": [],
      "source": [
        "class LinearProjectionLayer(nn.Module):\n",
        "  def __init__(self, d_model, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x) -> None:\n",
        "    return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ucsRWs0lG9"
      },
      "source": [
        "## The Transformer\n",
        "\n",
        "At this point, all we need to do is create a class that represents our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5ip11mmQ0nMM"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, encoder: EncoderBlock, decoder: DecoderBlock, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: LinearProjectionLayer) -> None:\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.projection_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embed(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "    tgt = self.tgt_embed(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "  def project(self, x):\n",
        "    return self.projection_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ssr5nA039--"
      },
      "source": [
        "## Building Our Transformer\n",
        "\n",
        "Now that we have each of our components - we need to construct an actual model!\n",
        "\n",
        "We'll use this helper function to aid in our goal and set up our Encoder/Decoder Stacks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kdCt3wNi4EvY"
      },
      "outputs": [],
      "source": [
        "def build_transformer(input_vocab_size: int, target_vocab_size: int, input_seq_len: int, target_seq_len: int, d_model: int=512, N: int=6, num_heads: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "  input_embeddings = InputEmbeddings(d_model, input_vocab_size)\n",
        "  target_embeddings = InputEmbeddings(d_model, target_vocab_size)\n",
        "\n",
        "  input_position = PositionalEncoding(d_model, input_seq_len, dropout)\n",
        "  target_position = PositionalEncoding(d_model, target_seq_len, dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    encoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "  decoder_blocks = []\n",
        "\n",
        "  for _ in range(N):\n",
        "    decoder_self_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    decoder_cross_attention_block = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "    decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder_stack = EncoderStack(d_model, nn.ModuleList(encoder_blocks))\n",
        "  decoder_stack = DecoderStack(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "  linear_projection_layer = LinearProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder_stack, decoder_stack, input_embeddings, target_embeddings, input_position, target_position, linear_projection_layer)\n",
        "\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2jfvmRx4ZNd"
      },
      "source": [
        "# Training Our Transformer!\n",
        "\n",
        "We will be using the resources created in [this](https://github.com/hkproj/pytorch-transformer/tree/main) repository to train our model on a English -> French translation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxF6JLcg5LJp"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "The BilingualDataset is a custom PyTorch dataset for working with translation data. It needs a tokenizer for each language, a dataset of sentence pairs, info on which languages are source and target, and the max sequence length.\n",
        "\n",
        "This class handles tokenizing the sentences, padding them to be the same length, and getting the data into the right format for sequence-to-sequence models. It adds special start, end, and padding tokens so all the inputs and outputs are the same length.\n",
        "\n",
        "When you grab a sample from the dataset, it tokenizes the source and target sentences, pads them, and creates the input tensors the model needs - encoder input, decoder input, and target labels. It also makes masks to show what's real data vs padding, and to make sure the decoder predictions only use previous tokens, not future ones.\n",
        "\n",
        "The BilingualDataset gets the data ready for training seq2seq models in a way that works with the sequential nature of language. The model can only predict the next token based on what came before it, not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WLnUZRgk7S-G"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "  def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "    super().__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "    self.ds = ds\n",
        "    self.tokenizer_src = tokenizer_src\n",
        "    self.tokenizer_tgt = tokenizer_tgt\n",
        "    self.src_lang = src_lang\n",
        "    self.tgt_lang = tgt_lang\n",
        "\n",
        "    self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "    self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "    self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src_target_pair = self.ds[idx]\n",
        "    src_text = src_target_pair['translation'][self.src_lang]\n",
        "    tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "    enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "    dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "    enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "    dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "    if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "        raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "    encoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    decoder_input = torch.cat(\n",
        "        [\n",
        "            self.sos_token,\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "        ],\n",
        "        dim=0,\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0) == self.seq_len\n",
        "    assert decoder_input.size(0) == self.seq_len\n",
        "    assert label.size(0) == self.seq_len\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,\n",
        "        \"decoder_input\": decoder_input,\n",
        "        \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "        \"label\": label,\n",
        "        \"src_text\": src_text,\n",
        "        \"tgt_text\": tgt_text,\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "  return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hSYI7F8IAI"
      },
      "source": [
        "## Build Tokenizer For Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfmUwnpo8qou",
        "outputId": "295cb9c8-c196-4a4b-bc0e-2d43721e6642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers datasets -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCDOGq0UD1W"
      },
      "source": [
        "This will grab all the sentences from our dataset per language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iAUUEXFz8eGH"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDKOCTVUIa7"
      },
      "source": [
        "We'll quickly train a tokenizer on our dataset for both our source and target languages.\n",
        "\n",
        "We'll be sure to add the `[UNK]`, `[PAD]`, `[SOS]`, and `[EOS]` special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4s1ZHzKb8hTN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "  tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "  tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEUDihkcVRq1"
      },
      "source": [
        "Now we can create our dataset in a format that our model expects and can train with!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yI7xXnLo84cE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_ds(config):\n",
        "  # It only has the train split, so we divide it overselves\n",
        "  ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "  # Build tokenizers\n",
        "  tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "  tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "  # Keep 90% for training, 10% for validation\n",
        "  train_ds_size = int(0.9 * len(ds_raw))\n",
        "  val_ds_size = len(ds_raw) - train_ds_size\n",
        "  train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "  train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "  val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "  # Find the maximum length of each sentence in the source and target sentence\n",
        "  max_len_src = 0\n",
        "  max_len_tgt = 0\n",
        "\n",
        "  for item in ds_raw:\n",
        "    src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "    tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "    max_len_src = max(max_len_src, len(src_ids))\n",
        "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "  print(f'Max length of source sentence: {max_len_src}')\n",
        "  print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3Ys-ufVW1E"
      },
      "source": [
        "We can build our model with this helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FK6k5X829JOo"
      },
      "outputs": [],
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "  model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JCALGXpf9tnv"
      },
      "outputs": [],
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "  return str(Path('.') / model_folder / model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QPWZgCwD9vVX"
      },
      "outputs": [],
      "source": [
        "def latest_weights_file_path(config):\n",
        "  model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "  model_filename = f\"{config['model_basename']}*\"\n",
        "  weights_files = list(Path(model_folder).glob(model_filename))\n",
        "  if len(weights_files) == 0:\n",
        "      return None\n",
        "  weights_files.sort()\n",
        "  return str(weights_files[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNjAS-LvV557"
      },
      "source": [
        "Finally....our training loop!\n",
        "\n",
        "We'll spend more time in following weeks discussing this - for now, we'll quickly walk through what's happening:\n",
        "\n",
        "1. Configure the training device (GPU/CPU) and print details. Set device in PyTorch.\n",
        "\n",
        "2. Create directory for saving model weights based on config.\n",
        "\n",
        "3. Get data loaders, tokenizers, and model. Move model to configured device.\n",
        "\n",
        "4. Initialize Adam optimizer with learning rate and epsilon from config.\n",
        "\n",
        "5. Set up initial training parameters like start epoch and global step.\n",
        "\n",
        "6. Define cross-entropy loss function with label smoothing, ignoring padding.\n",
        "\n",
        "---\n",
        "\n",
        "- Main training loop over epochs:\n",
        "\n",
        "  - Clear cache, set model to train mode, initialize progress bar.\n",
        "\n",
        "  - For each batch:\n",
        "\n",
        "    - Move data to device, run model forward/backward passes.\n",
        "    - Compute loss, backprop, update model weights.\n",
        "    - Increment global step.\n",
        "  - After each epoch, save model and optimizer checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gL1bH7is9OfS"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def train_model(config):\n",
        "  # Define the device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "  print(\"Using device:\", device)\n",
        "  if (device == 'cuda'):\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "  else:\n",
        "    print(\"Please ensure you're in a GPU enabled Colab Notebook instance.\")\n",
        "  device = torch.device(device)\n",
        "\n",
        "  # Make sure the weights folder exists\n",
        "  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "  model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "  initial_epoch = 0\n",
        "  global_step = 0\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "  for epoch in range(initial_epoch, config['num_epochs']):\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "    for batch in batch_iterator:\n",
        "      encoder_input = batch['encoder_input'].to(device)\n",
        "      decoder_input = batch['decoder_input'].to(device)\n",
        "      encoder_mask = batch['encoder_mask'].to(device)\n",
        "      decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "      encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "      decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "      proj_output = model.project(decoder_output)\n",
        "\n",
        "      label = batch['label'].to(device)\n",
        "\n",
        "      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'global_step': global_step\n",
        "    }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "f21pxGoS9-gM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  \"batch_size\": 16,\n",
        "  \"num_epochs\": 3,\n",
        "  \"lr\": 10**-4,\n",
        "  \"seq_len\": 350,\n",
        "  \"d_model\": 512,\n",
        "  \"datasource\": 'opus_books',\n",
        "  \"lang_src\": \"en\",\n",
        "  \"lang_tgt\": \"it\",\n",
        "  \"model_folder\": \"weights\",\n",
        "  \"model_basename\": \"encoder_decoder_model_\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "4f078e9b751a4da3b4e34fa85ffac4e3",
            "e1df974f064241bf81f187e97afb6191",
            "c65907be3b644900a0690a0fb1d3851f",
            "cc08e07e1d374c038edf69e71b239ca9",
            "6a1b3f4d5d644ac9b014996729e0a67b",
            "fd1257ea2170496494f009484945d443",
            "dbc163d8811b46f2a93139c9e8cd95d0",
            "fc96532f84504e129aadeca9687fb15a",
            "001d72445ae34c4eb4b91f2eff923a47",
            "f7517e44d90c47918e25033509a689fe",
            "a4f22c7a7b0e48ba8c7fb274941f0804",
            "24877132f55f463fbf70ae9384198a73",
            "dd5d5eace8d84416b07ce1806cbfffb1",
            "69550e9675f945c0a57ed4ce68fac1c9",
            "9dd6660499ca4c45b66737abb66c29b2",
            "d911aa75a0a440168222bad170a189bc",
            "a71d8f299fec45bd99c7a8079a71eff7",
            "6bc9958f6fd04ce6b261f405713553fc",
            "1e71d94a59754b24b52beec7ff080309",
            "f3995f7421a140bc903c82c45a1c4bd2",
            "1db1cc28e41347b7a82e6f09ef9b4208",
            "34e6b937a8184a8c944347008ca75f5f"
          ]
        },
        "id": "hfBcSHUo-MU6",
        "outputId": "2e2dd351-0e39-4996-9429-95ddb5801b3d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74810791015625 GB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f078e9b751a4da3b4e34fa85ffac4e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24877132f55f463fbf70ae9384198a73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1819/1819 [23:36<00:00,  1.28it/s, loss=5.974]\n",
            "Processing Epoch 01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1819/1819 [23:40<00:00,  1.28it/s, loss=5.547]\n",
            "Processing Epoch 02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1819/1819 [23:39<00:00,  1.28it/s, loss=4.927]\n"
          ]
        }
      ],
      "source": [
        "train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Cofqp53bQB"
      },
      "source": [
        "#### Acknowledgements\n",
        "\n",
        "This notebook is heavily adapted from a number of incredible resources on Transformers, including but not limited to:\n",
        "\n",
        "- https://blog.floydhub.com/the-transformer-in-pytorch/\n",
        "- https://arxiv.org/pdf/1706.03762.pdf\n",
        "- https://txt.cohere.com/what-are-transformer-models/\n",
        "- https://jalammar.github.io/illustrated-transformer/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f078e9b751a4da3b4e34fa85ffac4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1df974f064241bf81f187e97afb6191",
              "IPY_MODEL_c65907be3b644900a0690a0fb1d3851f",
              "IPY_MODEL_cc08e07e1d374c038edf69e71b239ca9"
            ],
            "layout": "IPY_MODEL_6a1b3f4d5d644ac9b014996729e0a67b"
          }
        },
        "e1df974f064241bf81f187e97afb6191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1257ea2170496494f009484945d443",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dbc163d8811b46f2a93139c9e8cd95d0",
            "value": "Downloading data: 100%"
          }
        },
        "c65907be3b644900a0690a0fb1d3851f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc96532f84504e129aadeca9687fb15a",
            "max": 5726189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_001d72445ae34c4eb4b91f2eff923a47",
            "value": 5726189
          }
        },
        "cc08e07e1d374c038edf69e71b239ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7517e44d90c47918e25033509a689fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a4f22c7a7b0e48ba8c7fb274941f0804",
            "value": " 5.73M/5.73M [00:00&lt;00:00, 7.83MB/s]"
          }
        },
        "6a1b3f4d5d644ac9b014996729e0a67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd1257ea2170496494f009484945d443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc163d8811b46f2a93139c9e8cd95d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc96532f84504e129aadeca9687fb15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "001d72445ae34c4eb4b91f2eff923a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7517e44d90c47918e25033509a689fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4f22c7a7b0e48ba8c7fb274941f0804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24877132f55f463fbf70ae9384198a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd5d5eace8d84416b07ce1806cbfffb1",
              "IPY_MODEL_69550e9675f945c0a57ed4ce68fac1c9",
              "IPY_MODEL_9dd6660499ca4c45b66737abb66c29b2"
            ],
            "layout": "IPY_MODEL_d911aa75a0a440168222bad170a189bc"
          }
        },
        "dd5d5eace8d84416b07ce1806cbfffb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71d8f299fec45bd99c7a8079a71eff7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6bc9958f6fd04ce6b261f405713553fc",
            "value": "Generating train split: 100%"
          }
        },
        "69550e9675f945c0a57ed4ce68fac1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e71d94a59754b24b52beec7ff080309",
            "max": 32332,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3995f7421a140bc903c82c45a1c4bd2",
            "value": 32332
          }
        },
        "9dd6660499ca4c45b66737abb66c29b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1db1cc28e41347b7a82e6f09ef9b4208",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34e6b937a8184a8c944347008ca75f5f",
            "value": " 32332/32332 [00:00&lt;00:00, 81484.57 examples/s]"
          }
        },
        "d911aa75a0a440168222bad170a189bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71d8f299fec45bd99c7a8079a71eff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc9958f6fd04ce6b261f405713553fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e71d94a59754b24b52beec7ff080309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3995f7421a140bc903c82c45a1c4bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1db1cc28e41347b7a82e6f09ef9b4208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e6b937a8184a8c944347008ca75f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}