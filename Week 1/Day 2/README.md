# Unsupervised Pre-Training of a GPT-Style Model

Today we'll be working through a notebook focused on tokenization, and unsupervised pre-training of a GPT-style, decoder only model. 

We'll be using Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) as our base model builder and architecture.

You can find Colab links to the assignments below:
- [Base Assignment](https://colab.research.google.com/drive/1usxOzWwLW9FDQxPetWi62PdRvx_JnMpJ?usp=sharing)
- [BEASTMODE Assignment](https://colab.research.google.com/drive/1DtE0Oi4KlD1jNOqsh-Gmq3xxOGAQV6F_?usp=sharing)

## Build üèóÔ∏è
We will train a GPT-style decoder-only model from scratch on Shakespeare data. You can find the notebook in this repository - or by following this link (or this one for the BEASTMODE assignment)

## Ship üö¢
Ship a completed notebook to us! There are a few questions found throughout the notebook that will need to be answered.

Provide a URL to your loom video walkthrough

## Share üöÄ

Share about your experience in a LinkedIn post, or in the Discord!

- Share 3 lessons learned 
- Share 3 lessons not yet learned